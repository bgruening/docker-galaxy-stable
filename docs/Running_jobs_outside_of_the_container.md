Using an external Slurm cluster
-------------------------------

It is often convenient to configure Galaxy to use a high-performance cluster for running jobs. To do so, two files are required:

 1. munge.key
 2. slurm.conf

These files from the cluster must be copied to the `/export` mount point (i.e., `/data/galaxy` on the host if using below command) accessible to Galaxy before starting the container. This must be done regardless of which Slurm daemons are running within Docker. At start, symbolic links will be created to these files to `/etc` within the container, allowing the various Slurm functions to communicate properly with your cluster. In such cases, there's no reason to run `slurmctld`, the Slurm controller daemon, from within Docker, so specify `-e "NONUSE=slurmctld"`. Unless you would like to also use Slurm (rather than the local job runner) to run jobs within the Docker container, then alternatively specify `-e "NONUSE=slurmctld,slurmd"`.

Importantly, Slurm relies on a shared filesystem between the Docker container and the execution nodes. To allow things to function correctly, each of the execution nodes will need `/export` and `/galaxy-central` directories to point to the appropriate places. Suppose you ran the following command to start the Docker image:

  ```sh
  docker run -d -e "NONUSE=slurmd,slurmctld" -p 80:80 -v /data/galaxy:/export bgruening/galaxy-stable
  ```

You would then need the following symbolic links on each of the nodes:

 1. `/export`  → `/data/galaxy`
 2. `/galaxy-central`  → `/data/galaxy/galaxy-central`

A brief note is in order regarding the version of Slurm installed. This Docker image uses Ubuntu 14.04 as its base image. The version of Slurm in the Unbuntu 14.04 repository is 2.6.5 and that is what is installed in this image. If your cluster is using an incompatible version of Slurm then you will likely need to modify this Docker image.

The following is an example for how to specify a destination in `job_conf.xml` that uses a custom partition ("work", rather than "debug") and 4 cores rather than 1:

```xml
    <destination id="slurm4threads" runner="slurm">
        <param id="embed_metadata_in_job">False</param>
        <param id="nativeSpecification">-p work -n 4</param>
    </destination>
```

The usage of `-n` can be confusing. Note that it will specify the number of cores, not the number of tasks (i.e., it's not equivalent to `srun -n 4`).

Tips for Running Jobs Outside the Container
---------------------------------------------

In its default state Galaxy assumes both the Galaxy source code and
various temporary files are available on shared file systems across the
cluster, and uses the Galaxy source code to calculate metadata about the
files that have been produced.
When using Condor or SLURM (as described above) to run jobs outside
of the Docker container one can disable the metadata generation on the cluster,
or synchronize the files required for generating these.

The ``embed_metadata_in_job`` option on job destinations in `job_conf.xml`
forces Galaxy collect metadata inside the container instead of on the
cluster:

```xml
    <param id="embed_metadata_in_job">False</param>
```

This has performance implications and may not scale as well as performing
these calculations on the remote cluster - but this should not be a problem
for most Galaxy instances.

Additionally, many framework tools depend on Galaxy's Python virtual
environment being avaiable. This should be created outside of the container
on a shared filesystem available to your cluster using the instructions
[here](https://github.com/galaxyproject/galaxy/blob/dev/doc/source/admin/framework_dependencies.rst#managing-dependencies-manually). Job destinations
can then source these virtual environments using the instructions outlined
[here](https://github.com/galaxyproject/galaxy/blob/dev/doc/source/admin/framework_dependencies.rst#galaxy-job-handlers). In other words, by adding
a line such as this to each job destination:

```xml
    <env file="/path/to/shared/galaxy/venv" />
```

A Hands-on example of running SLURM on an external cluster container
--------------------------------------------------------------------

In the [/test/slurm](../test/slurm/) folder you will find a Dockerfile
that can be used to build a SLURM docker image and to test the integration
of docker galaxy with SLURM.

To build the image, go the [/test/slurm](../test/slurm/) folder and type:
```sh
docker build -t slurm .
```
As explained above, to connect galaxy with the SLURM cluster, the slurm.conf
and munge.key files are needed. These file will be automatically generated by the
docker slurm container and placed into the /export folder.

To make them available to the galaxy container, we start the slurm container
with a host directory (`/data/galaxy`) mounted to `/export`.
(If there is a real cluster available, this would be a network share):
```sh
docker run -d -v /data/galaxy:/export \
           --name slurm \
           --hostname slurm \
           slurm
```
We are also using the `--hostname slurm`, which allows the galaxy container
to reach the slurm container use the `slurm` hostname.

You should see a `slurm.conf` and `munge.key` key file in the export folder.
We can now start and connect galaxy to the slurm cluster:
```sh
docker run -d -e "NONUSE=slurmd,slurmctld" \
           --name galaxy-slurm-test \
           --link slurm \
           -p 80:80 \
           -v /data/galaxy:/export \
           bgruening/galaxy-stable
```
Note the --link slurm, this will allow the galaxy container to talk to the slurm container.
On a real network this would not be necessary.
After a moment, we can enter the the docker container and submit a simple job using the srun utility:
```
docker exec galaxy-slurm-test srun hostname
```
This should return the hostname of the slurm container, slurm.
But we still need to instruct galaxy on how to interface with slurm.
We therefore need to adjust the job_conf.xml file.
A sample job_conf.xml is in [/test/slurm/job_conf.xml](../test/slurm/job_conf.xml).
We can copy this file to /data/galaxy/galaxy-central/config:
```
cp job_conf.xml /data/galaxy/galaxy-central/config
```
We restart galaxy inside the container
```sh
docker exec galaxy-slurm-test supervisorctl restart galaxy:
```

We should now be able to submit galaxy jobs through the slurm container.
To verify this you can install the printenv tools from the toolshed
(do not forget to restart galaxy after installing tools!)
and look at its output.

Bonus points
------------

In the [job_conf.xml](../test/slurm/job_conf.xml) we are disabling metadata generation
on the cluster, since this requires a set of galaxy's dependencies.
We can install these on the cluster, since the docker image copies galaxy's requirements.txt
and the galaxy's lib folder to /export.
We enter the slurm container and install these dependencies:

```sh
docker exec -it slurm bash
```
Inside the container we switch to the galaxy user, source the virtualenv, upgrade pip and install
the required dependencies:
```sh
source /galaxy-central/.venv/bin/activate && pip install --upgrade pip
pip install -r /galaxy-central/requirements.txt --index-url https://wheels.galaxyproject.org/simple
```
Now quit the slurm container, edit the job_conf.xml and set
```
<param id="embed_metadata_in_job">True</param>
```
and finally restart galaxy:
```
docker exec galaxy-slurm-test supervisorctl restart galaxy:
```
