from socket import gethostname
from string import Template
from os import environ
import psutil


SLURM_CONFIG_TEMPLATE = '''
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=$control_machine
#ControlAddr=
#BackupController=
#BackupAddr=
#
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
MpiDefault=none
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/pgid
#Prolog=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/tmp/slurmd
SlurmUser=$user
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/tmp/slurm
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/none
#TaskPluginParam=
#TaskProlog=
InactiveLimit=0
KillWait=30
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
FastSchedule=1
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
AccountingStorageType=accounting_storage/none
#AccountingStorageUser=
AccountingStoreJobComment=YES
ClusterName=$cluster_name
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=3
#SlurmctldLogFile=
SlurmdDebug=3
#SlurmdLogFile=
NodeName=$node_name NodeAddr=$hostname NodeHostname=$hostname CPUs=$cpus RealMemory=$memory State=UNKNOWN
PartitionName=$partition_name Nodes=$nodes Default=YES MaxTime=INFINITE State=UP Shared=YES
'''

try:
    # psutil version 0.2
    cpus = psutil.NUM_CPUS
    mem = psutil.TOTAL_PHYMEM
except:
    # psutil version 0.3
    cpus = psutil.cpu_count()
    mem = psutil.virtual_memory().total

def main():
    hostname = gethostname()
    node_name = environ.get('SLURM_NODE_NAME', hostname)
    template_params = {
        "hostname": hostname,
        "node_name": node_name,
        "nodes": ",".join(environ.get('SLURM_NODES', node_name).split(',')),
        "cluster_name": environ.get('SLURM_CLUSTER_NAME', 'Cluster'),
        "control_machine": environ.get('SLURM_CONTROL_MACHINE', hostname),
        "user": environ.get('SLURM_USER_NAME', '{{ galaxy_user_name }}'),
        "cpus": environ.get("SLURM_CPUS", cpus),
        "partition_name": environ.get('SLURM_PARTITION_NAME', 'debug'),
        "memory": environ.get("SLURM_MEMORY", int(mem / (1024 * 1024)))
    }
    config_contents = Template(SLURM_CONFIG_TEMPLATE).substitute(template_params)
    control_addr = environ.get('SLURM_CONTROL_ADDR', None)
    if control_addr:
        config_contents = config_contents.replace("#ControlAddr=", "ControlAddr=%s" % control_addr)
    # TODO: NodeAddr should probably galaxy-slurm in the Kubernetes case.
    open("/etc/slurm-llnl/slurm.conf", "w").write(config_contents)

if __name__ == "__main__":
    main()
